{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Program 3:\n",
    "#### Objective:\n",
    "Write a program using the PyTorch framework to highlight the use of BatchNormalization and\n",
    "Dropout Regularization techniques in CNNs on the CIFAR10 image dataset.\n",
    "Perform the following steps:\n",
    "- Preprocess data\n",
    "- Define CNN architecture with & without the use of BatchNormalization and Dropout\n",
    "- Define model train function\n",
    "- Train both CNNs using suitable criterion and optimizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data\\cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\cifar-10-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Data preprocessing and loading\n",
    "transform = transforms.Compose([\n",
    "transforms.ToTensor(),\n",
    "transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                                download=True, transform=transform)\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                            download=True, transform=transform)\n",
    "train_subset = Subset(train_dataset, range(200))\n",
    "test_subset = Subset(test_dataset, range(50))\n",
    "train_loader = DataLoader(train_subset, batch_size=10, shuffle=True)\n",
    "test_loader = DataLoader(test_subset, batch_size=10, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNWithBNDropout(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNWithBNDropout, self).__init__()\n",
    "        self.conv_block1 = nn.Sequential(\n",
    "        nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "        nn.BatchNorm2d(32),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.conv_block2 = nn.Sequential(\n",
    "        nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "        nn.BatchNorm2d(64),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.dense1 = nn.Linear(64 * 8 * 8, 512)\n",
    "        self.dense2 = nn.Linear(512, 10)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.flatten = nn.Flatten()\n",
    "    def forward(self, x):\n",
    "        x = self.conv_block1(x)\n",
    "        x = self.conv_block2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dense2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train and evaluate a model\n",
    "def train(model, optimizer, criterion, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "        for data, target in train_loader:\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            predicted = torch.argmax(output.data, dim=1)\n",
    "            total_train += target.size(0)\n",
    "            correct_train += (predicted == target).sum().item()\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        train_acc = 100 * correct_train / total_train\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        correct_test = 0\n",
    "        total_test = 0\n",
    "        with torch.no_grad():\n",
    "            for data, target in test_loader:\n",
    "                output = model(data)\n",
    "                loss = criterion(output, target)\n",
    "                test_loss += loss.item()\n",
    "                predicted = torch.argmax(output.data, dim=1)\n",
    "                total_test += target.size(0)\n",
    "                correct_test += (predicted==target).sum().item()\n",
    "        avg_test_loss = test_loss/len(test_loader)\n",
    "        test_acc = 100 * correct_test/total_test\n",
    "        print(f'Epoch: [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f},Train Accuracy: {train_acc:.4f}%, Test Loss: {avg_test_loss:.4f}, Test Accuracy: {test_acc:.4f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNNWithBNDropout()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/30], Train Loss: 0.8999,Train Accuracy: 57.5000%, Test Loss: 2.6193, Test Accuracy: 16.0000%\n",
      "Epoch: [2/30], Train Loss: 0.8077,Train Accuracy: 59.5000%, Test Loss: 2.7801, Test Accuracy: 18.0000%\n",
      "Epoch: [3/30], Train Loss: 0.7717,Train Accuracy: 62.0000%, Test Loss: 2.6042, Test Accuracy: 24.0000%\n",
      "Epoch: [4/30], Train Loss: 0.9060,Train Accuracy: 55.5000%, Test Loss: 2.8748, Test Accuracy: 18.0000%\n",
      "Epoch: [5/30], Train Loss: 0.7975,Train Accuracy: 64.5000%, Test Loss: 2.7386, Test Accuracy: 20.0000%\n",
      "Epoch: [6/30], Train Loss: 0.7950,Train Accuracy: 61.0000%, Test Loss: 2.6173, Test Accuracy: 22.0000%\n",
      "Epoch: [7/30], Train Loss: 0.7988,Train Accuracy: 62.0000%, Test Loss: 2.7926, Test Accuracy: 18.0000%\n",
      "Epoch: [8/30], Train Loss: 0.8412,Train Accuracy: 62.0000%, Test Loss: 2.9674, Test Accuracy: 16.0000%\n",
      "Epoch: [9/30], Train Loss: 0.8599,Train Accuracy: 62.5000%, Test Loss: 2.9301, Test Accuracy: 18.0000%\n",
      "Epoch: [10/30], Train Loss: 0.7809,Train Accuracy: 65.0000%, Test Loss: 2.7958, Test Accuracy: 18.0000%\n",
      "Epoch: [11/30], Train Loss: 0.7877,Train Accuracy: 62.0000%, Test Loss: 3.0317, Test Accuracy: 18.0000%\n",
      "Epoch: [12/30], Train Loss: 0.8188,Train Accuracy: 61.5000%, Test Loss: 3.0778, Test Accuracy: 20.0000%\n",
      "Epoch: [13/30], Train Loss: 0.7949,Train Accuracy: 61.0000%, Test Loss: 2.9242, Test Accuracy: 16.0000%\n",
      "Epoch: [14/30], Train Loss: 0.7771,Train Accuracy: 63.0000%, Test Loss: 2.8224, Test Accuracy: 18.0000%\n",
      "Epoch: [15/30], Train Loss: 0.8102,Train Accuracy: 60.5000%, Test Loss: 2.8971, Test Accuracy: 16.0000%\n",
      "Epoch: [16/30], Train Loss: 0.8077,Train Accuracy: 60.0000%, Test Loss: 2.9772, Test Accuracy: 18.0000%\n",
      "Epoch: [17/30], Train Loss: 0.7825,Train Accuracy: 63.5000%, Test Loss: 3.0321, Test Accuracy: 18.0000%\n",
      "Epoch: [18/30], Train Loss: 0.7325,Train Accuracy: 65.5000%, Test Loss: 3.1090, Test Accuracy: 18.0000%\n",
      "Epoch: [19/30], Train Loss: 0.8644,Train Accuracy: 55.0000%, Test Loss: 3.0094, Test Accuracy: 22.0000%\n",
      "Epoch: [20/30], Train Loss: 0.8068,Train Accuracy: 64.5000%, Test Loss: 2.9527, Test Accuracy: 18.0000%\n",
      "Epoch: [21/30], Train Loss: 0.8362,Train Accuracy: 59.0000%, Test Loss: 2.9402, Test Accuracy: 18.0000%\n",
      "Epoch: [22/30], Train Loss: 0.8427,Train Accuracy: 60.0000%, Test Loss: 3.0028, Test Accuracy: 18.0000%\n",
      "Epoch: [23/30], Train Loss: 0.8227,Train Accuracy: 63.0000%, Test Loss: 2.9182, Test Accuracy: 18.0000%\n",
      "Epoch: [24/30], Train Loss: 0.9766,Train Accuracy: 52.5000%, Test Loss: 2.9128, Test Accuracy: 18.0000%\n",
      "Epoch: [25/30], Train Loss: 0.8679,Train Accuracy: 57.0000%, Test Loss: 2.9727, Test Accuracy: 18.0000%\n",
      "Epoch: [26/30], Train Loss: 0.8451,Train Accuracy: 55.5000%, Test Loss: 3.0317, Test Accuracy: 18.0000%\n",
      "Epoch: [27/30], Train Loss: 0.8452,Train Accuracy: 60.0000%, Test Loss: 3.0884, Test Accuracy: 16.0000%\n",
      "Epoch: [28/30], Train Loss: 0.9019,Train Accuracy: 54.0000%, Test Loss: 3.0876, Test Accuracy: 16.0000%\n",
      "Epoch: [29/30], Train Loss: 0.8882,Train Accuracy: 59.0000%, Test Loss: 3.0204, Test Accuracy: 16.0000%\n",
      "Epoch: [30/30], Train Loss: 0.8212,Train Accuracy: 61.0000%, Test Loss: 3.1538, Test Accuracy: 16.0000%\n"
     ]
    }
   ],
   "source": [
    "train(model, optimizer, criterion, 30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
