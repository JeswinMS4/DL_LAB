{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNwF+Mp/CgUtWHkX+JL5sY4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#### Program 9:\n","##### Objective:\n","  Implement an AutoEncoder using the PyTorch framework.\n","\n","Tasks:\n","  - Implement an AutoEncoder architecture.\n","  - Preprocess the dataset.\n","  - Define a model train function.\n","  - Train the model using suitable criterion and optimizer.\n"],"metadata":{"id":"bI8Aok2tHi43"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3IKxUydtHci5","executionInfo":{"status":"ok","timestamp":1720623141720,"user_tz":-330,"elapsed":19622,"user":{"displayName":"JESWIN M S","userId":"17267416076578851849"}},"outputId":"2de82211-7177-4ac1-fe0f-d7e66c63815a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n","Failed to download (trying next):\n","HTTP Error 403: Forbidden\n","\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 9912422/9912422 [00:00<00:00, 50394435.00it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n","Failed to download (trying next):\n","HTTP Error 403: Forbidden\n","\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 28881/28881 [00:00<00:00, 1750389.33it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n","Failed to download (trying next):\n","HTTP Error 403: Forbidden\n","\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1648877/1648877 [00:00<00:00, 13954780.23it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n","Failed to download (trying next):\n","HTTP Error 403: Forbidden\n","\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 4542/4542 [00:00<00:00, 1295249.44it/s]"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n","\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1, Train Loss: 0.1351, Test Loss: 0.0743\n","Epoch 2, Train Loss: 0.0745, Test Loss: 0.0708\n","Epoch 3, Train Loss: 0.0701, Test Loss: 0.0690\n","Epoch 4, Train Loss: 0.0643, Test Loss: 0.0628\n","Epoch 5, Train Loss: 0.0575, Test Loss: 0.0567\n","Epoch 6, Train Loss: 0.0515, Test Loss: 0.0527\n","Epoch 7, Train Loss: 0.0458, Test Loss: 0.0486\n","Epoch 8, Train Loss: 0.0405, Test Loss: 0.0454\n","Epoch 9, Train Loss: 0.0369, Test Loss: 0.0430\n","Epoch 10, Train Loss: 0.0337, Test Loss: 0.0418\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchvision import datasets, transforms\n","from torch.utils.data import DataLoader, Subset\n","\n","transform = transforms.Compose([\n","    transforms.ToTensor()\n","])\n","\n","train_dataset = datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n","test_dataset = datasets.MNIST(root=\"./data\", train=False, download=True, transform=transform)\n","\n","train_subset = Subset(train_dataset, range(200))\n","test_subset = Subset(test_dataset, range(50))\n","\n","train_loader = DataLoader(train_subset, batch_size=10, shuffle=True)\n","test_loader = DataLoader(test_subset, batch_size=10, shuffle=False)\n","\n","class AutoEncoder(nn.Module):\n","    def __init__(self):\n","        super(AutoEncoder, self).__init__()\n","        self.encoder = nn.Sequential(\n","            nn.Linear(28*28, 256),\n","            nn.ReLU(inplace=True),\n","            nn.Linear(256, 64),\n","        )\n","        self.decoder = nn.Sequential(\n","            nn.Linear(64, 256),\n","            nn.ReLU(inplace=True),\n","            nn.Linear(256, 28*28),\n","            nn.Sigmoid()\n","        )\n","\n","    def forward(self, x):\n","        x = self.encoder(x)\n","        x = self.decoder(x)\n","        return x\n","\n","model = AutoEncoder()\n","optimizer = optim.Adam(model.parameters())\n","criterion = nn.MSELoss()\n","\n","def train_model(num_epochs):\n","    for epoch in range(num_epochs):\n","        model.train()\n","        train_loss = 0.0\n","        for data in train_loader:\n","            img, _ = data\n","            img = img.view(img.size(0), -1)\n","            output = model(img)\n","            loss = criterion(output, img)\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","            train_loss += loss.item()\n","\n","        avg_train_loss = train_loss / len(train_loader)\n","\n","        model.eval()\n","        test_loss = 0.0\n","        with torch.no_grad():\n","            for data in test_loader:\n","                img, _ = data\n","                img = img.view(img.size(0), -1)\n","                output = model(img)\n","                loss = criterion(output, img)\n","                test_loss += loss.item()\n","\n","        avg_test_loss = test_loss / len(test_loader)\n","        print(f'Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}, Test Loss: {avg_test_loss:.4f}')\n","\n","train_model(10)\n"]}]}