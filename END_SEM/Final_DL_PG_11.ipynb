{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM/oBzu14wvsmlXF8tk6CAw"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#### Program 11:\n","#### Objective:\n","  Implement the Self-Attention mechanism using the PyTorch framework.\n","\n","Tasks:\n","  - Define the Self-Attention mechanism.\n","  - Show the forward pass.\n"],"metadata":{"id":"SlDL2J5HQqYk"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t8U9dBnxQovU","executionInfo":{"status":"ok","timestamp":1720625480932,"user_tz":-330,"elapsed":9082,"user":{"displayName":"JESWIN M S","userId":"17267416076578851849"}},"outputId":"d09ab080-0042-41d8-b22e-8b3fb054a3a7"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([32, 10, 128])\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class SelfAttention(nn.Module):\n","    def __init__(self, embed_dim, num_heads):\n","        super(SelfAttention, self).__init__()\n","        self.embed_dim = embed_dim\n","        self.num_heads = num_heads\n","        assert embed_dim % num_heads == 0, \"Embedding dimension must be 0 modulo number of heads\"\n","        self.head_dim = embed_dim // num_heads\n","        self.scale = self.head_dim ** -0.5\n","        self.query = nn.Linear(embed_dim, embed_dim)\n","        self.key = nn.Linear(embed_dim, embed_dim)\n","        self.value = nn.Linear(embed_dim, embed_dim)\n","        self.out = nn.Linear(embed_dim, embed_dim)\n","\n","    def forward(self, x):\n","        batch_size, seq_len, embed_dim = x.size()\n","        # Compute Q, K, V matrices\n","        Q = self.query(x)  # (batch_size, seq_len, embed_dim)\n","        K = self.key(x)    # (batch_size, seq_len, embed_dim)\n","        V = self.value(x)  # (batch_size, seq_len, embed_dim)\n","\n","        # Split the embedding into multiple heads\n","        Q = Q.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n","        K = K.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n","        V = V.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n","\n","        # Compute attention scores\n","        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) * self.scale\n","        attn_weights = F.softmax(attn_scores, dim=-1)\n","\n","        # Compute the weighted sum of the values\n","        attn_output = torch.matmul(attn_weights, V)\n","\n","        # Concatenate the multiple heads\n","        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len, embed_dim)\n","\n","        # Apply the final linear layer\n","        output = self.out(attn_output)\n","        return output\n","\n","embed_dim = 128\n","num_heads = 8\n","seq_len = 10\n","batch_size = 32\n","\n","x = torch.randn(batch_size, seq_len, embed_dim)\n","self_attention = SelfAttention(embed_dim, num_heads)\n","output = self_attention(x)\n","print(output.shape)  # Output shape will be (batch_size, seq_len, embed_dim)\n"]}]}