{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOlVOxmrNowwvXx12TIt0dg"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#### Program 10:\n","##### Objective:\n","  Implement a Generative Adversarial Network (GAN) on the MNIST dataset using the PyTorch framework.\n","\n","Tasks:\n","  - Define a GAN architecture.\n","  - Preprocess the MNIST dataset.\n","  - Define the model train function.\n","  - Train the model using suitable criterion and optimizer.\n"],"metadata":{"id":"tYs1F3zKPrWS"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cNHU4PjnPkWg","executionInfo":{"status":"ok","timestamp":1720625242351,"user_tz":-330,"elapsed":35056,"user":{"displayName":"JESWIN M S","userId":"17267416076578851849"}},"outputId":"16115c8f-2301-4957-b55d-c9bafc390cab"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n","Failed to download (trying next):\n","HTTP Error 403: Forbidden\n","\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 9912422/9912422 [00:00<00:00, 51911433.36it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n","Failed to download (trying next):\n","HTTP Error 403: Forbidden\n","\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 28881/28881 [00:00<00:00, 1619721.00it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n","Failed to download (trying next):\n","HTTP Error 403: Forbidden\n","\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1648877/1648877 [00:00<00:00, 13196956.40it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n","Failed to download (trying next):\n","HTTP Error 403: Forbidden\n","\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 4542/4542 [00:00<00:00, 1591389.92it/s]"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n","\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1, Loss D: 0.5309, Loss G: 1.1956\n","Epoch 2, Loss D: 0.2890, Loss G: 2.1580\n","Epoch 3, Loss D: 0.4902, Loss G: 1.6557\n","Epoch 4, Loss D: 0.4337, Loss G: 1.2396\n","Epoch 5, Loss D: 0.4178, Loss G: 1.6804\n","Epoch 6, Loss D: 0.4102, Loss G: 1.5597\n","Epoch 7, Loss D: 0.3869, Loss G: 1.3843\n","Epoch 8, Loss D: 0.4317, Loss G: 1.5284\n","Epoch 9, Loss D: 0.3976, Loss G: 1.3018\n","Epoch 10, Loss D: 0.2943, Loss G: 1.4342\n","Epoch 11, Loss D: 0.3589, Loss G: 1.1268\n","Epoch 12, Loss D: 0.4174, Loss G: 0.9415\n","Epoch 13, Loss D: 0.5020, Loss G: 0.7116\n","Epoch 14, Loss D: 0.4995, Loss G: 0.6887\n","Epoch 15, Loss D: 0.4828, Loss G: 0.7063\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchvision import datasets, transforms\n","from torch.utils.data import DataLoader, Subset\n","\n","transform = transforms.Compose([\n","    transforms.ToTensor()\n","])\n","\n","dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n","subset = Subset(dataset, range(1000))\n","dataloader = DataLoader(subset, batch_size=10, shuffle=True)\n","\n","class Generator(nn.Module):\n","    def __init__(self):\n","        super(Generator, self).__init__()\n","        self.gen = nn.Sequential(\n","            nn.Linear(100, 256),\n","            nn.ReLU(),\n","            nn.Linear(256, 28*28),\n","            nn.Tanh()\n","        )\n","\n","    def forward(self, x):\n","        return self.gen(x)\n","\n","class Discriminator(nn.Module):\n","    def __init__(self):\n","        super(Discriminator, self).__init__()\n","        self.disc = nn.Sequential(\n","            nn.Linear(28*28, 256),\n","            nn.ReLU(),\n","            nn.Linear(256, 1),\n","            nn.Sigmoid()\n","        )\n","\n","    def forward(self, x):\n","        return self.disc(x)\n","\n","generator = Generator()\n","discriminator = Discriminator()\n","criterion = nn.BCELoss()\n","optim_gen = optim.Adam(generator.parameters(), lr=2e-4)\n","optim_disc = optim.Adam(discriminator.parameters(), lr=2e-4)\n","\n","def train(num_epochs):\n","    for epoch in range(num_epochs):\n","        generator.train()\n","        discriminator.train()\n","        for real, _ in dataloader:\n","            real = real.view(-1, 28*28)\n","            batch_size = real.size(0)\n","\n","            # Train Discriminator\n","            noise = torch.randn(batch_size, 100)\n","            fake = generator(noise)\n","            disc_real = discriminator(real)\n","            loss_disc_real = criterion(disc_real, torch.ones_like(disc_real))\n","            disc_fake = discriminator(fake)\n","            loss_disc_fake = criterion(disc_fake, torch.zeros_like(disc_fake))\n","            loss_disc = (loss_disc_real + loss_disc_fake) / 2\n","\n","            # Backprop\n","            optim_disc.zero_grad()\n","            loss_disc.backward()\n","            optim_disc.step()\n","\n","            # Train Generator\n","            noise = torch.randn(batch_size, 100)\n","            fake = generator(noise)\n","            disc_fake = discriminator(fake)\n","            loss_gen = criterion(disc_fake, torch.ones_like(disc_fake))\n","\n","            # Backprop\n","            optim_gen.zero_grad()\n","            loss_gen.backward()\n","            optim_gen.step()\n","\n","        print(f'Epoch {epoch+1}, Loss D: {loss_disc.item():.4f}, Loss G: {loss_gen.item():.4f}')\n","\n","train(15)\n"]}]}