{"cells":[{"cell_type":"markdown","metadata":{"id":"jpguIG3s5t1Z"},"source":["\n","#### Program 4:\n","\n","##### Objective:\n","Write a program to implement the SGD and Adagrad optimizers using the PyTorch framework, and compare results using the MNIST digit classification dataset. Use a simple CNN to illustrate the difference between the two optimizers.\n","\n","Perform the following steps:\n","1. **Preprocess data**\n","2. **Define SGD and Adagrad optimizers from scratch**\n","3. **Define a simple CNN model architecture**\n","4. **Train CNN model using suitable criterion and each optimizer**"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KTEs4NzM5t1c","executionInfo":{"status":"ok","timestamp":1720619350043,"user_tz":-330,"elapsed":19816,"user":{"displayName":"JESWIN M S","userId":"17267416076578851849"}},"outputId":"418a4346-0442-431b-aad4-ba7e2d67db4d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n","Failed to download (trying next):\n","HTTP Error 403: Forbidden\n","\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 9912422/9912422 [00:00<00:00, 15919254.19it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n","Failed to download (trying next):\n","HTTP Error 403: Forbidden\n","\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 28881/28881 [00:00<00:00, 479168.42it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n","Failed to download (trying next):\n","HTTP Error 403: Forbidden\n","\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1648877/1648877 [00:00<00:00, 4359355.87it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n","Failed to download (trying next):\n","HTTP Error 403: Forbidden\n","\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 4542/4542 [00:00<00:00, 1815892.55it/s]"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n","\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torchvision import datasets, transforms\n","from torch.utils.data import DataLoader, Subset\n","from torch.optim import Optimizer\n","\n","# Data preprocessing\n","transform = transforms.Compose([\n","    transforms.ToTensor()\n","])\n","\n","train_dataset = datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n","test_dataset = datasets.MNIST(root=\"./data\", train=False, download=True, transform=transform)\n","\n","train_subset = Subset(train_dataset, range(200))\n","test_subset = Subset(test_dataset, range(50))\n","\n","train_loader = DataLoader(train_subset, batch_size=10, shuffle=True)\n","test_loader = DataLoader(test_subset, batch_size=10, shuffle=False)\n","\n","# Simple CNN model definition\n","class SimpleCNN(nn.Module):\n","    def __init__(self):\n","        super(SimpleCNN, self).__init__()\n","        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n","        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n","        self.fc1 = nn.Linear(320, 50)\n","        self.fc2 = nn.Linear(50, 10)\n","\n","    def forward(self, x):\n","        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n","        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n","        x = x.view(-1, 320)\n","        x = F.relu(self.fc1(x))\n","        x = self.fc2(x)\n","        return F.log_softmax(x, dim=1)\n","\n","# SGD update function\n","def sgd_update(parameters, lr):\n","    with torch.no_grad():\n","        for param in parameters:\n","            if param.grad is not None:\n","                param.data -= lr * param.grad.data\n","                param.grad.zero_()\n","\n","# Custom Adagrad optimizer\n","class CustomAdagrad(Optimizer):\n","    def __init__(self, parameters, lr=0.01, epsilon=1e-10):\n","        self.parameters = list(parameters)\n","        self.lr = lr\n","        self.epsilon = epsilon\n","        self.sum_squared_gradients = [torch.zeros_like(p) for p in self.parameters]\n","\n","    def step(self):\n","        with torch.no_grad():\n","            for param, sum_sq_grad in zip(self.parameters, self.sum_squared_gradients):\n","                if param.grad is not None:\n","                    sum_sq_grad += param.grad.data ** 2\n","                    adjusted_lr = self.lr / (self.epsilon + torch.sqrt(sum_sq_grad))\n","                    param.data -= adjusted_lr * param.grad.data\n","                    param.grad.zero_()\n","\n","    def zero_grad(self):\n","        with torch.no_grad():\n","            for param in self.parameters:\n","                if param.grad is not None:\n","                    param.grad.zero_()\n","\n","# Training setup\n","device = torch.device('cpu')\n","model = SimpleCNN().to(device)\n","criterion = nn.CrossEntropyLoss()\n","\n","# Training function\n","def train_model(num_epochs, optimizer_choice='adagrad'):\n","    if optimizer_choice == 'sgd':\n","        optimizer = None\n","    else:\n","        optimizer = CustomAdagrad(model.parameters(), lr=0.01)\n","\n","    for epoch in range(num_epochs):\n","        model.train()\n","        train_loss = 0\n","        correct_train = 0\n","        total_train = 0\n","        for data, target in train_loader:\n","            data, target = data.to(device), target.to(device)\n","            optimizer.zero_grad()\n","            output = model(data)\n","            loss = criterion(output, target)\n","            loss.backward()\n","            if optimizer_choice == 'sgd':\n","                sgd_update(model.parameters(), lr=0.01)\n","            else:\n","                optimizer.step()\n","            train_loss += loss.item()\n","            predicted = torch.argmax(output.data, dim=1)\n","            total_train += target.size(0)\n","            correct_train += (predicted == target).sum().item()\n","        avg_train_loss = train_loss / len(train_loader)\n","        train_acc = 100 * correct_train / total_train\n","\n","        model.eval()\n","        test_loss = 0\n","        correct_test = 0\n","        total_test = 0\n","        with torch.no_grad():\n","            for data, target in test_loader:\n","                output = model(data)\n","                loss = criterion(output, target)\n","                test_loss += loss.item()\n","                predicted = torch.argmax(output.data, dim=1)\n","                total_test += target.size(0)\n","                correct_test += (predicted == target).sum().item()\n","        avg_test_loss = test_loss / len(test_loader)\n","        test_acc = 100 * correct_test / total_test\n","\n","        print(f'Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}, Train Accuracy: {train_acc:.8f}%, '\n","              f'Test Loss: {avg_test_loss:.4f}, Test Accuracy: {test_acc:.8f}%')\n","\n"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J5gctztI5t1d","executionInfo":{"status":"ok","timestamp":1720619351245,"user_tz":-330,"elapsed":1224,"user":{"displayName":"JESWIN M S","userId":"17267416076578851849"}},"outputId":"07b0de04-2103-4e16-f2e6-d8fad052f288"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1, Train Loss: 2.2271, Train Accuracy: 19.50000000%, Test Loss: 1.8507, Test Accuracy: 58.00000000%\n","Epoch 2, Train Loss: 1.4540, Train Accuracy: 57.00000000%, Test Loss: 1.0847, Test Accuracy: 68.00000000%\n","Epoch 3, Train Loss: 0.7972, Train Accuracy: 80.00000000%, Test Loss: 0.7910, Test Accuracy: 76.00000000%\n","Epoch 4, Train Loss: 0.4681, Train Accuracy: 87.00000000%, Test Loss: 0.7475, Test Accuracy: 78.00000000%\n","Epoch 5, Train Loss: 0.3615, Train Accuracy: 89.00000000%, Test Loss: 0.6567, Test Accuracy: 88.00000000%\n"]}],"source":["\n","# Train the model\n","train_model(5, optimizer_choice='adagrad')"]}],"metadata":{"kernelspec":{"display_name":".venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"},"orig_nbformat":4,"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}